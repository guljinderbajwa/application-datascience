{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa678eee",
   "metadata": {},
   "source": [
    "## Fine tuning for emprical privacy testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f25e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import nltk\n",
    "import json\n",
    "import string\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import wn\n",
    "nltk.download('words', quiet=True)\n",
    "nltk.download(\"punkt\", quiet=True)\n",
    "nltk.download(\"stopwords\", quiet=True)\n",
    "nltk.download(\"words\", quiet=True)\n",
    "nltk.download('averaged_perceptron_tagger', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from transformers import AutoModel, AutoTokenizer, AutoModelForMaskedLM, logging\n",
    "import importlib_resources as impresources\n",
    "\n",
    "wn.download('oewn:2022')\n",
    "en = wn.Wordnet('oewn:2022')\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "logging.set_verbosity_warning()\n",
    "\n",
    "stop = set([x for x in stopwords.words(\"english\")])\n",
    "\n",
    "def nth_repl(s, sub, repl, n):\n",
    "    s_split = s.split()\n",
    "    i = 0\n",
    "    try:\n",
    "        find = s_split.index(sub)\n",
    "        i += 1\n",
    "    except ValueError:\n",
    "        return s\n",
    "\n",
    "    while i != n:\n",
    "        try:\n",
    "            find = s_split.index(sub, find + 1)\n",
    "            i += 1\n",
    "        except ValueError:\n",
    "            break\n",
    "    if i == n:\n",
    "        return \" \".join(s_split[:find] + [repl] + s_split[find+1:])\n",
    "    return s\n",
    "\n",
    "def nth_rem(s, sub, n):\n",
    "    s_split = s.split()\n",
    "    i = 0\n",
    "    try:\n",
    "        find = s_split.index(sub)\n",
    "        i += 1\n",
    "    except ValueError:\n",
    "        return s\n",
    "\n",
    "    while i != n:\n",
    "        try:\n",
    "            find = s_split.index(sub, find + 1)\n",
    "            i += 1\n",
    "        except ValueError:\n",
    "            break\n",
    "    if i == n:\n",
    "        return \" \".join(s_split[:find] + s_split[find+1:])\n",
    "    return s\n",
    "\n",
    "def sentence_enum(tokens):\n",
    "    counts = Counter()\n",
    "    n = []\n",
    "    for t in tokens:\n",
    "        counts[t] += 1\n",
    "        n.append(counts[t])\n",
    "    return n\n",
    "\n",
    "def get_opposites():\n",
    "\twith open(\"/content/drive/MyDrive/dpmlm_data/opposites.json\", 'r') as f:\n",
    "\t\topposites = json.load(f)\n",
    "\treturn opposites\n",
    "\n",
    "def get_vocab():\n",
    "\twith open(\"/content/drive/MyDrive/dpmlm_data/vocab.txt\", 'r',encoding=\"utf-8\") as f:\n",
    "\t\tvocab = set([x.strip() for x in f.readlines()])\n",
    "\treturn vocab\n",
    "\n",
    "def get_antonyms(word):\n",
    "    ants = list()\n",
    "\n",
    "    #Get antonyms from WordNet for this word and any of its synonyms.\n",
    "    for ss in en.synsets(word):\n",
    "        for sense in ss.senses():\n",
    "            ants.extend([x.word().lemma() for x in sense.get_related(\"antonym\")])\n",
    "\n",
    "    #Get snyonyms of antonyms found in the previous step, thus expanding the list even more.\n",
    "    syns = list()\n",
    "    for word in list(set(ants)):\n",
    "        for ss in en.synsets(word):\n",
    "            syns.extend(ss.lemmas())\n",
    "\n",
    "    return sorted(list(set(syns)))\n",
    "\n",
    "'''\n",
    "Gets pertainyms of the target word from the WordNet knowledge base.\n",
    "* pertainyms = words pertaining to the target word (industrial -> pertainym is \"industry\")\n",
    "'''\n",
    "def get_pertainyms(word):\n",
    "    perts = list()\n",
    "    for ss in en.synsets(word):\n",
    "        for sense in ss.senses():\n",
    "            perts.extend([x.word().lemma() for x in sense.get_related(\"pertainym\")])\n",
    "    return sorted(list(set(perts)))\n",
    "'''\n",
    "Get hyponyms (new wn)\n",
    "'''\n",
    "def get_hyponyms(word):\n",
    "    hypo = list()\n",
    "    for ss in en.synsets(word):\n",
    "        for sense in ss.senses():\n",
    "            hypo.extend([x.word().lemma() for x in sense.get_related(\"hyponyms\")])\n",
    "    return sorted(list(set(hypo)))\n",
    "\n",
    "'''\n",
    "Get hypernyms (new wn)\n",
    "'''\n",
    "def get_hypernyms(word):\n",
    "    hyper = list()\n",
    "    for ss in en.synsets(word):\n",
    "        for h in ss.hypernyms():\n",
    "            hyper.extend([x.lemma() for x in h.words()])\n",
    "    return sorted(list(set(hyper)))\n",
    "\n",
    "'''\n",
    "Gets derivationally related forms (e.g. begin -> 'beginner', 'beginning')\n",
    "'''\n",
    "def get_related_forms(word):\n",
    "    forms = list()\n",
    "    for ss in wn.synsets(word):\n",
    "        for sense in ss.senses():\n",
    "            forms.extend([x.word().lemma() for x in sense.get_related(\"derivation\")])\n",
    "    return sorted(list(set(forms)))\n",
    "\n",
    "'''\n",
    "General get nym\n",
    "'''\n",
    "def get_general_nym(word, nym):\n",
    "    n = list()\n",
    "    for ss in wn.synsets(word):\n",
    "        for sense in ss.senses():\n",
    "            n.extend([x.word().lemma() for x in sense.get_related(nym)])\n",
    "    return sorted(list(set(n)))\n",
    "\n",
    "'''\n",
    "Gets antonyms, hypernyms, hyponyms, holonyms, meronyms, pertainyms, and derivationally related forms of a target word from WordNet.\n",
    "* hypernym = a word whose meaning includes a group of other words (\"animal\" is a hypernym of \"dog\")\n",
    "* hyponym = a word whose meaning is included in the meaning of another word (\"bulldog\" is a hyponym of \"dog\")\n",
    "* a meronym denotes a part and a holonym denotes a whole: \"week\" is a holonym of \"weekend\", \"eye\" is a meronym of \"face\", and vice-versa\n",
    "'''\n",
    "def get_nyms(word, depth=-1):\n",
    "    nym_list = ['antonyms', 'hypernyms', 'hyponyms', 'holonyms', 'meronyms',\n",
    "                'pertainyms', 'derivationally_related_forms']\n",
    "    results = list()\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    word = lemmatizer.lemmatize(word)\n",
    "\n",
    "    def query_wordnet(getter):\n",
    "        res = list()\n",
    "        for ss in en.synsets(word):\n",
    "            res_list = [item.lemmas() for item in ss.closure(getter)]\n",
    "            res_list = [item.name() for sublist in res_list for item in sublist]\n",
    "            res.extend(res_list)\n",
    "        return res\n",
    "\n",
    "    for nym in nym_list:\n",
    "        if nym=='antonyms':\n",
    "            results.append(get_antonyms(word))\n",
    "\n",
    "        elif nym == \"hypernyms\":\n",
    "            results.append(get_hypernyms(word))\n",
    "\n",
    "        elif nym == \"hyponyms\":\n",
    "            results.append(get_hyponyms(word))\n",
    "\n",
    "        elif nym in ['holonyms', 'meronyms']:\n",
    "            res = list()\n",
    "            #Three different types of holonyms and meronyms as defined in WordNet\n",
    "            for postfix in [\"_member\", \"_part\", \"_portion\", \"_substance\"]:\n",
    "                res.extend(get_general_nym(word, \"{}{}\".format(nym[:4], postfix)))\n",
    "            results.append(res)\n",
    "\n",
    "        elif nym=='pertainyms':\n",
    "            results.append(get_pertainyms(word))\n",
    "\n",
    "        else:\n",
    "            results.append(get_related_forms(word))\n",
    "\n",
    "    results = map(set, results)\n",
    "    nyms = dict(zip(nym_list, results))\n",
    "    return nyms\n",
    "\n",
    "#Converts a part-of-speech tag returned by NLTK to a POS tag from WordNet\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wn.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wn.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wn.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wn.ADV\n",
    "    else:\n",
    "        return ''\n",
    "\n",
    "#Function for clearing up duplicate words (capitalized, upper-case, etc.), stop words, and antonyms from the list of candidates.\n",
    "def filter_words(target, words, scr, tkn, opp={}):\n",
    "    dels = list()\n",
    "    toks = tkn.tolist()\n",
    "    nyms = get_nyms(target)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    if lemmatizer.lemmatize(target.lower()) in opp:\n",
    "        opp_del = [x for x in words if lemmatizer.lemmatize(x.lower()) in opp[lemmatizer.lemmatize(target.lower())]]\n",
    "        dels.extend(opp_del)\n",
    "\n",
    "    for w in words:\n",
    "        if w.lower() in words and w.upper() in words:\n",
    "            dels.append(w.upper())\n",
    "        if lemmatizer.lemmatize(w.lower()) in nyms['antonyms']:\n",
    "            dels.append(w)\n",
    "\n",
    "    dels = list(set(dels))\n",
    "    for d in dels:\n",
    "        del scr[words.index(d)]\n",
    "        del toks[words.index(d)]\n",
    "        words.remove(d)\n",
    "\n",
    "    return words, scr, torch.tensor(toks)\n",
    "\n",
    "#Calculates the similarity score\n",
    "def similarity_score(original_output, subst_output, k):\n",
    "    mask_idx = k\n",
    "    cos_sim = torch.nn.CosineSimilarity(dim=1, eps=1e-6)\n",
    "    weights = torch.div(torch.stack(list(original_output[3])).squeeze().sum(0).sum(0), (12 * 12.0))\n",
    "\n",
    "    suma = 0.0\n",
    "    sent_len = original_output[2][2].shape[1]\n",
    "\n",
    "    for token_idx in range(sent_len):\n",
    "        original_hidden = original_output[2]\n",
    "        subst_hidden = subst_output[2]\n",
    "\n",
    "        #Calculate the contextualized representation of the i-th word as a concatenation of RoBERTa's values in its last four layers\n",
    "        context_original = torch.cat( tuple( [original_hidden[hs_idx][:, token_idx, :] for hs_idx in [1, 2, 3, 4]] ), dim=1)\n",
    "        context_subst = torch.cat( tuple( [subst_hidden[hs_idx][:, token_idx, :] for hs_idx in [1, 2, 3, 4]] ), dim=1)\n",
    "        suma += weights[mask_idx][token_idx] * cos_sim(context_original, context_subst)\n",
    "\n",
    "    substitute_validation = suma\n",
    "    return substitute_validation\n",
    "\n",
    "#Calculates the proposal score\n",
    "def proposal_score(original_score, subst_scores, device):\n",
    "    subst_scores = torch.tensor(subst_scores).to(device)\n",
    "    return np.log( torch.div(subst_scores , (1.0 - original_score)).cpu() )\n",
    "\n",
    "class DPMLM():\n",
    "    opposites = get_opposites()\n",
    "    vocab = get_vocab()\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    detokenizer = TreebankWordDetokenizer()\n",
    "    tokenizer = None\n",
    "    lm_model = None\n",
    "    raw_model = None\n",
    "    device = None\n",
    "    nlp = None\n",
    "    alpha = None\n",
    "\n",
    "    def __init__(self, MODEL=\"roberta-base\", SPACY=\"en_core_web_md\", alpha=0.003):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "        self.lm_model = AutoModelForMaskedLM.from_pretrained(MODEL)\n",
    "        self.raw_model = AutoModel.from_pretrained(MODEL, output_hidden_states=True, output_attentions=True)\n",
    "        self.alpha = alpha\n",
    "\n",
    "        self.clip_min = -3.2093127\n",
    "        self.clip_max = 16.304797887802124\n",
    "        self.sensitivity = abs(self.clip_max - self.clip_min)\n",
    "\n",
    "        self.device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.lm_model = self.lm_model.to(self.device)\n",
    "        self.raw_model = self.raw_model.to(self.device)\n",
    "\n",
    "    def load_transformers(self):\n",
    "        return self.tokenizer, self.lm_model, self.raw_model\n",
    "\n",
    "    #Calculates the proposal scores, substitute validation scores, and then the final score for each candidate word's fit as a substitution.\n",
    "    def calc_scores(self, scr, sentences, original_output, original_score, mask_index):\n",
    "        #Get representations of all substitute sentences\n",
    "        _, _, raw_model = self.load_transformers()\n",
    "        subst_output = raw_model(sentences)\n",
    "\n",
    "        prop_score = proposal_score(original_score, scr, self.device)\n",
    "        substitute_validation = similarity_score(original_output, subst_output, mask_index)\n",
    "\n",
    "        final_score = substitute_validation.cpu() + self.alpha*prop_score\n",
    "\n",
    "        return final_score, prop_score, substitute_validation\n",
    "\n",
    "    def privatize(self, sentence, target, n=1, K=5, CONCAT=True, FILTER=True, POS=False, ENGLISH=False, epsilon=1, MS=None, TEMP=False):\n",
    "        split_sent = nltk.word_tokenize(sentence)\n",
    "        original_sent = ' '.join(split_sent)\n",
    "        #orig_pos = [x.tag_ for x in self.nlp(original_sent)]\n",
    "\n",
    "        # Masks the target word in the original sentence.\n",
    "        if MS is None:\n",
    "            masked_sent = ' '.join(split_sent)\n",
    "        else:\n",
    "            masked_sent = MS\n",
    "\n",
    "        if isinstance(target, list):\n",
    "            if n == 1:\n",
    "                n = [1 for _ in range(len(target))]\n",
    "\n",
    "            for t, nn in zip(target, n):\n",
    "                masked_sent = nth_repl(masked_sent, t, self.tokenizer.mask_token, nn)\n",
    "        else:\n",
    "            masked_sent = nth_repl(masked_sent, target, self.tokenizer.mask_token, n)\n",
    "            n = [n]\n",
    "\n",
    "        #Get the input token IDs of the input consisting of: the original sentence + separator + the masked sentence.\n",
    "        if CONCAT == False:\n",
    "            input_ids = self.tokenizer.encode(\" \"+masked_sent, add_special_tokens=True)\n",
    "        else:\n",
    "            input_ids = self.tokenizer.encode(\" \"+original_sent.replace(\"MASK\", \"\"), \" \"+masked_sent, add_special_tokens=True)\n",
    "        if isinstance(target, list):\n",
    "            masked_position = np.where(np.array(input_ids) == self.tokenizer.mask_token_id)[0].tolist()\n",
    "        else:\n",
    "            masked_position = [input_ids.index(self.tokenizer.mask_token_id)]\n",
    "            target = [target]\n",
    "\n",
    "        original_output = self.raw_model(torch.tensor(input_ids).reshape(1, len(input_ids)).to(self.device))\n",
    "\n",
    "        #Get the predictions of the Masked LM transformer.\n",
    "        with torch.no_grad():\n",
    "            output = self.lm_model(torch.tensor(input_ids).reshape(1, len(input_ids)).to(self.device))\n",
    "\n",
    "        logits = output[0].squeeze().detach().cpu().numpy()\n",
    "\n",
    "        predictions = {}\n",
    "        for t, m, nn in zip(target, masked_position, n):\n",
    "            current = \"{}_{}\".format(t, nn)\n",
    "\n",
    "            #Get top guesses: their token IDs, scores, and words.\n",
    "            mask_logits = logits[m].squeeze()\n",
    "            if TEMP == True:\n",
    "                mask_logits = np.clip(mask_logits, self.clip_min, self.clip_max)\n",
    "                mask_logits = mask_logits / (2 * self.sensitivity / epsilon)\n",
    "\n",
    "                logits_idx = [i for i, x in enumerate(mask_logits)]\n",
    "                scores = torch.softmax(torch.from_numpy(mask_logits), dim=0)\n",
    "                scores = scores / scores.sum()\n",
    "                chosen_idx = np.random.choice(logits_idx, p=scores.numpy())\n",
    "                predictions[current] = (self.tokenizer.decode(chosen_idx).strip(), scores[chosen_idx])\n",
    "                continue\n",
    "            else:\n",
    "                top_tokens = torch.topk(torch.from_numpy(mask_logits), k=K, dim=0)[1]\n",
    "                scores = torch.softmax(torch.from_numpy(mask_logits), dim=0)[top_tokens].tolist()\n",
    "            words = [self.tokenizer.decode(i.item()).strip() for i in top_tokens]\n",
    "\n",
    "            if FILTER == True:\n",
    "                words, scores, top_tokens = filter_words(t, words, scores, top_tokens, self.opposites)\n",
    "\n",
    "            if len(words) == 0:\n",
    "                predictions[current] = [(t, 1)]\n",
    "                continue\n",
    "\n",
    "\n",
    "            assert len(words) == len(scores)\n",
    "\n",
    "            if len(words) == 0:\n",
    "                predictions[current] = [(t, 1)]\n",
    "                continue\n",
    "\n",
    "            original_score = torch.softmax(torch.from_numpy(mask_logits), dim=0)[m]\n",
    "            sentences = list()\n",
    "\n",
    "            for i in range(len(words)):\n",
    "                subst_word = top_tokens[i]\n",
    "                input_ids[m] = int(subst_word)\n",
    "                sentences.append(list(input_ids))\n",
    "\n",
    "            torch_sentences = torch.tensor(sentences).to(self.device)\n",
    "\n",
    "            finals, _, _ = self.calc_scores(scores, torch_sentences, original_output, original_score, m)\n",
    "            finals = map(lambda f : float(f), finals)\n",
    "\n",
    "            zipped = dict(zip(words, finals))\n",
    "            for i in range(len(words)):\n",
    "                cand = words[i]\n",
    "                if cand not in zipped:\n",
    "                    continue\n",
    "\n",
    "                # remove non-words\n",
    "                if ENGLISH == True:\n",
    "                    if cand not in self.vocab and self.lemmatizer.lemmatize(cand) not in self.vocab:\n",
    "                        del zipped[cand]\n",
    "                        continue\n",
    "\n",
    "            zipped = dict(zipped)\n",
    "            finish = list(sorted(zipped.items(), key=lambda item: item[1], reverse=True))[:K]\n",
    "            predictions[current] = finish\n",
    "\n",
    "        if TEMP == True:\n",
    "            for p in predictions:\n",
    "                predictions[p] = predictions[p][0]\n",
    "\n",
    "        return predictions\n",
    "\n",
    "    def dpmlm_rewrite(self, sentence, epsilon, REPLACE=False, FILTER=False, STOP=False, TEMP=True, POS=True, CONCAT=True):\n",
    "        if isinstance(sentence, list):\n",
    "            tokens = sentence\n",
    "        else:\n",
    "            tokens = nltk.word_tokenize(sentence)\n",
    "\n",
    "        if isinstance(epsilon, list):\n",
    "            word_eps = epsilon\n",
    "        else:\n",
    "            word_eps = [epsilon for _ in range(len(tokens))] #epsilon #/ num_tokens\n",
    "        n = sentence_enum(tokens)\n",
    "        replace = []\n",
    "        new_tokens = [str(x) for x in tokens]\n",
    "\n",
    "        perturbed = 0\n",
    "        total = 0\n",
    "        for i, (t, nn, eps) in enumerate(zip(tokens, n, word_eps)):\n",
    "            if i >= len(tokens):\n",
    "                break\n",
    "\n",
    "            if (STOP == False and t in stop) or t in string.punctuation:\n",
    "                total += 1\n",
    "                if tokens[i][0].isupper() == True:\n",
    "                    replace.append(t.capitalize())\n",
    "                else:\n",
    "                    replace.append(t)\n",
    "                continue\n",
    "\n",
    "            if REPLACE == True:\n",
    "                new_s = \" \".join(new_tokens)\n",
    "                new_n = sentence_enum(new_tokens)\n",
    "                res = self.privatize(sentence, t, n=new_n[i], ENGLISH=True, FILTER=FILTER, epsilon=eps, MS=new_s, TEMP=TEMP, POS=POS, CONCAT=CONCAT)\n",
    "                r = res[t+\"_{}\".format(new_n[i])]\n",
    "                new_tokens[i] = r\n",
    "            else:\n",
    "                res = self.privatize(sentence, t, n=nn, ENGLISH=True, FILTER=FILTER, epsilon=eps, TEMP=TEMP, POS=POS, CONCAT=CONCAT)\n",
    "                r = res[t+\"_{}\".format(nn)]\n",
    "\n",
    "            if tokens[i][0].isupper() == True:\n",
    "                replace.append(r.capitalize())\n",
    "            else:\n",
    "                replace.append(r.lower())\n",
    "\n",
    "            if r != t:\n",
    "                perturbed += 1\n",
    "            total += 1\n",
    "\n",
    "        return self.detokenizer.detokenize(replace), perturbed, total\n",
    "\n",
    "    def dpmlm_rewrite_plus(self, sentence, epsilon, FILTER=False, TEMP=True, POS=True, CONCAT=True, ADD_PROB=0.15, DEL_PROB=0.05):\n",
    "        if isinstance(sentence, list):\n",
    "            tokens = sentence\n",
    "        else:\n",
    "            tokens = nltk.word_tokenize(sentence)\n",
    "\n",
    "        if isinstance(epsilon, list):\n",
    "            word_eps = epsilon\n",
    "        else:\n",
    "            word_eps = [epsilon for _ in range(len(tokens))] #epsilon #/ num_tokens\n",
    "        n = sentence_enum(tokens)\n",
    "        replace = []\n",
    "        new_tokens = [str(x) for x in tokens]\n",
    "\n",
    "        perturbed = 0\n",
    "        total = 0\n",
    "        deleted = 0\n",
    "        added = 0\n",
    "\n",
    "        for i, (t, nn, eps) in enumerate(zip(tokens, n, word_eps)):\n",
    "            if t in string.punctuation:\n",
    "                total += 1\n",
    "                replace.append(t)\n",
    "                continue\n",
    "\n",
    "            if i == len(tokens) - 1:\n",
    "                DELETE = 1\n",
    "            else:\n",
    "                DELETE = np.random.rand()\n",
    "            if DELETE >= DEL_PROB:\n",
    "                new_s = \" \".join(new_tokens)\n",
    "                new_n = sentence_enum(new_tokens)\n",
    "                res = self.privatize(sentence, t, n=new_n[i+added-deleted], ENGLISH=True, FILTER=FILTER, epsilon=eps, MS=new_s, TEMP=TEMP, POS=POS, CONCAT=CONCAT)\n",
    "                r = res[t+\"_{}\".format(new_n[i+added-deleted])]\n",
    "                if i+added-deleted > len(new_tokens) - 1:\n",
    "                    new_tokens.insert(i+added-deleted, r)\n",
    "                else:\n",
    "                    new_tokens[i+added-deleted] = r\n",
    "                replace.append(r)\n",
    "\n",
    "                if r != t:\n",
    "                    perturbed += 1\n",
    "                total += 1\n",
    "            else:\n",
    "                new_n = sentence_enum(new_tokens)\n",
    "                temp = nth_rem(\" \".join(new_tokens), t, new_n[i+added-deleted])\n",
    "                new_tokens = [str(x) for x in temp.split()]\n",
    "                deleted += 1\n",
    "                continue\n",
    "\n",
    "            ADD = np.random.rand()\n",
    "            if ADD <= ADD_PROB:\n",
    "                tokens_copy = new_tokens.copy()\n",
    "                tokens_copy.insert(i+1+added-deleted, \"MASK\")\n",
    "                new_s = \" \".join(tokens_copy)\n",
    "                new_n = sentence_enum(new_tokens)\n",
    "                res = self.privatize(sentence, \"MASK\", n=1, ENGLISH=True, FILTER=FILTER, epsilon=eps, MS=new_s, TEMP=TEMP, POS=POS, CONCAT=CONCAT)\n",
    "                r = res[\"MASK_1\"]\n",
    "                new_tokens.insert(i+1+added-deleted, r)\n",
    "                replace.append(r)\n",
    "                added += 1\n",
    "\n",
    "        return self.detokenizer.detokenize(replace), perturbed, total, added, deleted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5284c2cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional installs (uncomment if needed)\n",
    "# !pip install -q transformers datasets evaluate accelerate scikit-learn\n",
    "\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, Dict, Any\n",
    "\n",
    "import evaluate\n",
    "from datasets import load_dataset, DatasetDict, Value\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    DataCollatorWithPadding,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# 0) REQUIRED: your privatizers\n",
    "# -----------------------------\n",
    "# M, N, K must exist and expose:\n",
    "#   M.dpmlm_rewrite(text: str, epsilon: int) -> str\n",
    "#   N.Privatize(text: str, epsilon: int) -> str\n",
    "#   K.privatize(text: str, epsilon: int) -> str\n",
    "#\n",
    "# Example placeholder (NO-OP). Replace with your real ones.\n",
    "# class MClass:  def dpmlm_rewrite(self, s, eps): return s\n",
    "# class NClass:  def Privatize(self, s, eps):    return s\n",
    "# class KClass:  def privatize(self, s, eps):    return s\n",
    "# M, N, K = MClass(), NClass(), KClass()\n",
    "#\n",
    "# >>> Replace these with your actual implementations:\n",
    "M, N, K = M, N, K  # assuming you already constructed them\n",
    "\n",
    "# If your methods rely on NLTK tokenizers, ensure they’re available.\n",
    "try:\n",
    "    import nltk\n",
    "    for res in [\"punkt\", \"punkt_tab\"]:\n",
    "        try: nltk.data.find(f\"tokenizers/{res}\")\n",
    "        except LookupError: nltk.download(res)\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# -----------------------------\n",
    "# 1) Config\n",
    "# -----------------------------\n",
    "MODEL_UTIL = \"microsoft/deberta-v3-base\"   # utility model (fine-tune)\n",
    "MODEL_ADV  = \"distilbert-base-uncased\"     # adversary BERT (fast & small)\n",
    "\n",
    "MAX_LEN = 128\n",
    "BATCH   = 16\n",
    "LR      = 5e-5\n",
    "EPOCHS  = 3\n",
    "WD      = 0.01\n",
    "SEED    = 42\n",
    "\n",
    "OUTPUT_BASE = \"exp_dp_privacy_utility\"\n",
    "EPSILONS    = [10, 25, 50, 100, 250]\n",
    "TECHNIQUES  = [\"DPMLM\", \"DP-PARAPHRASE\", \"DP-PROMPT\"]  # M, N, K\n",
    "\n",
    "random.seed(SEED); np.random.seed(SEED)\n",
    "\n",
    "# Two datasets + their text columns; label column name\n",
    "# GLUE/SST-2: sentence + label (0/1)\n",
    "# IMDB: text + label (0/1)\n",
    "DATASETS = [\n",
    "    (\"glue\", \"sst2\", \"sentence\", None, \"label\"),\n",
    "    (\"imdb\", None,  \"text\",     None, \"label\"),\n",
    "]\n",
    "\n",
    "# -----------------------------\n",
    "# 2) Utility helpers\n",
    "# -----------------------------\n",
    "def make_training_args(**base_kwargs):\n",
    "    \"\"\"Shim: transformers v5 uses eval_strategy, v4 uses evaluation_strategy.\"\"\"\n",
    "    try:\n",
    "        return TrainingArguments(eval_strategy=\"epoch\", **base_kwargs)\n",
    "    except TypeError:\n",
    "        return TrainingArguments(evaluation_strategy=\"epoch\", **base_kwargs)\n",
    "\n",
    "def _to_str(x):\n",
    "    if x is None: return \"\"\n",
    "    return x if isinstance(x, str) else str(x)\n",
    "\n",
    "def privatize_fn_builder(tech: str, eps: int, col1: str, col2: Optional[str]):\n",
    "    \"\"\"Return a batched map function that rewrites text columns.\"\"\"\n",
    "    def fn(batch):\n",
    "        out = {}\n",
    "        if tech == \"DPMLM\":\n",
    "            out[col1] = [ _to_str(M.dpmlm_rewrite(_to_str(t), eps)) for t in batch[col1] ]\n",
    "        elif tech == \"DP-PARAPHRASE\":\n",
    "            out[col1] = [ _to_str(N.Privatize(_to_str(t), eps))      for t in batch[col1] ]\n",
    "        elif tech == \"DP-PROMPT\":\n",
    "            out[col1] = [ _to_str(K.privatize(_to_str(t), eps))      for t in batch[col1] ]\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown tech {tech}\")\n",
    "\n",
    "        if col2 is not None and col2 in batch:\n",
    "            if tech == \"DPMLM\":\n",
    "                out[col2] = [ _to_str(M.dpmlm_rewrite(_to_str(t), eps)) for t in batch[col2] ]\n",
    "            elif tech == \"DP-PARAPHRASE\":\n",
    "                out[col2] = [ _to_str(N.Privatize(_to_str(t), eps))      for t in batch[col2] ]\n",
    "            elif tech == \"DP-PROMPT\":\n",
    "                out[col2] = [ _to_str(K.privatize(_to_str(t), eps))      for t in batch[col2] ]\n",
    "\n",
    "        # keep label\n",
    "        if \"label\" in batch:\n",
    "            out[\"label\"] = batch[\"label\"]\n",
    "        return out\n",
    "    return fn\n",
    "\n",
    "def tokenize_builder(tok, col1, col2, label_col):\n",
    "    \"\"\"Tokenize & attach labels -> 'labels' field.\"\"\"\n",
    "    def fn(batch):\n",
    "        if col2 is None:\n",
    "            enc = tok(batch[col1], truncation=True, max_length=MAX_LEN)\n",
    "        else:\n",
    "            enc = tok(batch[col1], batch[col2], truncation=True, max_length=MAX_LEN)\n",
    "        enc[\"labels\"] = batch[label_col] if label_col in batch else batch[\"label\"]\n",
    "        return enc\n",
    "    return fn\n",
    "\n",
    "# Utility/accuracy metric\n",
    "acc_metric = evaluate.load(\"accuracy\")\n",
    "def compute_accuracy(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = logits.argmax(axis=-1)\n",
    "    return acc_metric.compute(predictions=preds, references=labels)\n",
    "\n",
    "# -----------------------------\n",
    "# 3) Adversarial “protected attribute”\n",
    "# -----------------------------\n",
    "# For a concrete, reproducible demo, we define a *proxy* protected attribute:\n",
    "# y_prot = 1 if text length (in whitespace tokens) >= dataset median, else 0\n",
    "# Replace `derive_protected(batch, col1)` with your real attribute labels if you have them.\n",
    "def derive_protected(batch, text_col: str):\n",
    "    lens = [len(_to_str(t).split()) for t in batch[text_col]]\n",
    "    return {\"protected_raw_len\": lens}\n",
    "\n",
    "def add_protected_label(dset, text_col: str):\n",
    "    # compute length per split\n",
    "    dset = dset.map(lambda b: derive_protected(b, text_col), batched=True)\n",
    "    # get median from train split\n",
    "    median_len = np.median(dset[\"train\"][\"protected_raw_len\"])\n",
    "    def bucket(b):\n",
    "        return {\"protected\": [1 if l >= median_len else 0 for l in b[\"protected_raw_len\"]]}\n",
    "    dset = dset.map(bucket, batched=True)\n",
    "    return dset\n",
    "\n",
    "# Attacker A: TF-IDF + Logistic Regression\n",
    "def run_attacker_tfidf(train_texts, train_y, val_texts, val_y):\n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.pipeline import Pipeline\n",
    "\n",
    "    pipe = Pipeline([\n",
    "        (\"tfidf\", TfidfVectorizer(max_features=50000, ngram_range=(1,2))),\n",
    "        (\"clf\",   LogisticRegression(max_iter=1000, n_jobs=None))\n",
    "    ])\n",
    "    pipe.fit(train_texts, train_y)\n",
    "    pred = pipe.predict(val_texts)\n",
    "    acc = (pred == np.array(val_y)).mean().item()\n",
    "    return {\"advA_accuracy\": acc}\n",
    "\n",
    "# Attacker B: small transformer classifier (DistilBERT) on protected label\n",
    "def run_attacker_bert(train_texts, train_y, val_texts, val_y):\n",
    "    adv_tok = AutoTokenizer.from_pretrained(MODEL_ADV, use_fast=True)\n",
    "\n",
    "    def to_ds(texts, labels):\n",
    "        import pandas as pd\n",
    "        from datasets import Dataset\n",
    "        df = pd.DataFrame({\"text\": texts, \"label\": labels})\n",
    "        return Dataset.from_pandas(df)\n",
    "\n",
    "    ds_train = to_ds(train_texts, train_y)\n",
    "    ds_val   = to_ds(val_texts,   val_y)\n",
    "\n",
    "    def tok(b):\n",
    "        enc = adv_tok(b[\"text\"], truncation=True, max_length=MAX_LEN)\n",
    "        enc[\"labels\"] = b[\"label\"]\n",
    "        return enc\n",
    "\n",
    "    ds_train = ds_train.map(tok, batched=True, remove_columns=ds_train.column_names)\n",
    "    ds_val   = ds_val.map(tok,   batched=True, remove_columns=ds_val.column_names)\n",
    "\n",
    "    adv_model = AutoModelForSequenceClassification.from_pretrained(MODEL_ADV, num_labels=2)\n",
    "    args = make_training_args(\n",
    "        output_dir=f\"{OUTPUT_BASE}-advB\",\n",
    "        save_strategy=\"no\",\n",
    "        learning_rate=5e-5,\n",
    "        per_device_train_batch_size=32,\n",
    "        per_device_eval_batch_size=32,\n",
    "        num_train_epochs=1,      # keep adversary light\n",
    "        weight_decay=0.0,\n",
    "        logging_dir=f\"{OUTPUT_BASE}-advB/logs\",\n",
    "        report_to=\"none\"\n",
    "    )\n",
    "    trainer = Trainer(\n",
    "        model=adv_model,\n",
    "        args=args,\n",
    "        train_dataset=ds_train,\n",
    "        eval_dataset=ds_val,\n",
    "        tokenizer=adv_tok,\n",
    "        data_collator=DataCollatorWithPadding(adv_tok),\n",
    "        compute_metrics=compute_accuracy\n",
    "    )\n",
    "    trainer.train()\n",
    "    res = trainer.evaluate()\n",
    "    return {\"advB_accuracy\": res.get(\"eval_accuracy\", None)}\n",
    "\n",
    "# -----------------------------\n",
    "# 4) Main loop: datasets × techniques × epsilons\n",
    "# -----------------------------\n",
    "def main():\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_UTIL, use_fast=True)\n",
    "    collator  = DataCollatorWithPadding(tokenizer)\n",
    "\n",
    "    all_results = []\n",
    "\n",
    "    for (name, subset, text1, text2, label_col) in DATASETS:\n",
    "        print(f\"\\n================ Dataset: {name}{('/' + subset) if subset else ''} ================\\n\")\n",
    "\n",
    "        # Load dataset\n",
    "        if name == \"glue\" and subset == \"sst2\":\n",
    "            ds = load_dataset(name, subset)\n",
    "            ds = DatasetDict({\"train\": ds[\"train\"], \"validation\": ds[\"validation\"]})\n",
    "        elif name == \"imdb\":\n",
    "            ds = load_dataset(\"imdb\")\n",
    "            ds = DatasetDict({\"train\": ds[\"train\"], \"validation\": ds[\"test\"]})\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported dataset: {name}/{subset}\")\n",
    "\n",
    "        # Arrow schema safety — force text columns to string\n",
    "        ds = ds.cast_column(text1, Value(\"string\"))\n",
    "        if text2 is not None and text2 in ds[\"train\"].column_names:\n",
    "            ds = ds.cast_column(text2, Value(\"string\"))\n",
    "\n",
    "        # Add synthetic protected label (replace with your own if you have it)\n",
    "        ds = add_protected_label(ds, text1)\n",
    "\n",
    "        # Utility label space size:\n",
    "        label_feat = ds[\"train\"].features[label_col]\n",
    "        num_labels = len(label_feat.names) if hasattr(label_feat, \"names\") and label_feat.names else 2\n",
    "\n",
    "        for tech in TECHNIQUES:\n",
    "            for eps in EPSILONS:\n",
    "                tag = f\"{name}{('-' + subset) if subset else ''}-{tech}-eps{eps}\"\n",
    "                print(f\"\\n---- {tag} ----\")\n",
    "\n",
    "                # 1) Privatize\n",
    "                priv_fn = privatize_fn_builder(tech, eps, text1, text2)\n",
    "                ds_priv = DatasetDict({\n",
    "                    \"train\": ds[\"train\"].map(priv_fn, batched=True),\n",
    "                    \"validation\": ds[\"validation\"].map(priv_fn, batched=True),\n",
    "                })\n",
    "\n",
    "                # 2) Utility: fine-tune DeBERTa-v3-base for sentiment/label prediction\n",
    "                token_fn = tokenize_builder(tokenizer, text1, text2, label_col)\n",
    "                remove_cols = [c for c in ds_priv[\"train\"].column_names if c not in {text1, text2, label_col, \"protected\", \"protected_raw_len\"}]\n",
    "                enc = DatasetDict({\n",
    "                    \"train\": ds_priv[\"train\"].map(token_fn, batched=True, remove_columns=remove_cols),\n",
    "                    \"validation\": ds_priv[\"validation\"].map(token_fn, batched=True, remove_columns=remove_cols),\n",
    "                })\n",
    "\n",
    "                util_model = AutoModelForSequenceClassification.from_pretrained(MODEL_UTIL, num_labels=num_labels)\n",
    "                args = make_training_args(\n",
    "                    output_dir=f\"{OUTPUT_BASE}-{tag}\",\n",
    "                    save_strategy=\"no\",\n",
    "                    learning_rate=LR,\n",
    "                    per_device_train_batch_size=BATCH,\n",
    "                    per_device_eval_batch_size=BATCH,\n",
    "                    num_train_epochs=EPOCHS,\n",
    "                    weight_decay=WD,\n",
    "                    logging_dir=f\"{OUTPUT_BASE}-{tag}/logs\",\n",
    "                    report_to=\"none\",\n",
    "                    load_best_model_at_end=False,\n",
    "                    logging_steps=100,\n",
    "                    # fp16=True,  # uncomment if your GPU supports it\n",
    "                )\n",
    "                trainer = Trainer(\n",
    "                    model=util_model,\n",
    "                    args=args,\n",
    "                    train_dataset=enc[\"train\"],\n",
    "                    eval_dataset=enc[\"validation\"],\n",
    "                    tokenizer=tokenizer,\n",
    "                    data_collator=collator,\n",
    "                    compute_metrics=compute_accuracy,\n",
    "                )\n",
    "                trainer.train()\n",
    "                util_res = trainer.evaluate()\n",
    "                util_acc = util_res.get(\"eval_accuracy\", None)\n",
    "\n",
    "                # 3) Privacy: adversaries try to predict protected label from privatized text\n",
    "                # Prepare raw privatized texts + protected labels for attackers\n",
    "                tr_text = ds_priv[\"train\"][text1]\n",
    "                va_text = ds_priv[\"validation\"][text1]\n",
    "                tr_yprot = ds[\"train\"][\"protected\"]\n",
    "                va_yprot = ds[\"validation\"][\"protected\"]\n",
    "\n",
    "                # Attacker A (TF-IDF + LR)\n",
    "                advA = run_attacker_tfidf(tr_text, tr_yprot, va_text, va_yprot)\n",
    "\n",
    "                # Attacker B (DistilBERT)\n",
    "                advB = run_attacker_bert(tr_text, tr_yprot, va_text, va_yprot)\n",
    "\n",
    "                row = {\n",
    "                    \"dataset\": f\"{name}{('/' + subset) if subset else ''}\",\n",
    "                    \"technique\": tech,\n",
    "                    \"epsilon\": eps,\n",
    "                    \"utility_accuracy\": util_acc,\n",
    "                    **advA,\n",
    "                    **advB,\n",
    "                }\n",
    "                all_results.append(row)\n",
    "                print(\"RESULT:\", row)\n",
    "\n",
    "    # Final compact table\n",
    "    try:\n",
    "        import pandas as pd\n",
    "        df = pd.DataFrame(all_results)\n",
    "        print(\"\\n=== SUMMARY ===\\n\", df.to_string(index=False))\n",
    "        # df.to_csv(\"summary_results.csv\", index=False)\n",
    "    except Exception:\n",
    "        print(\"\\n=== SUMMARY (raw) ===\\n\", all_results)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
