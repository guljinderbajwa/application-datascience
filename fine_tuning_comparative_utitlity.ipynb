{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3098e26b",
   "metadata": {},
   "source": [
    "## Fine tuining modal with dp_prompt and dp-paraphraser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c51719",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class code for LLM-based Differential Privacy mechanisms\n",
    "\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, AutoModelForCausalLM, LogitsProcessor, LogitsProcessorList, pipeline\n",
    "from transformers import BartTokenizer, BartModel, BartForConditionalGeneration\n",
    "from transformers.models.bart.modeling_bart import BartDecoder\n",
    "import torch\n",
    "import torch.nn.functional as nnf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "from pathlib import Path\n",
    "\n",
    "import mpmath\n",
    "from mpmath import mp\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt', quiet=True)\n",
    "\n",
    "class ClipLogitsProcessor(LogitsProcessor):\n",
    "  def __init__(self, min=-100, max=100):\n",
    "    self.min = min\n",
    "    self.max = max\n",
    "\n",
    "  def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor) -> torch.FloatTensor:\n",
    "    scores = torch.clamp(scores, min=self.min, max=self.max)\n",
    "    return scores\n",
    "  \n",
    "class DPPrompt():\n",
    "    model_checkpoint = None\n",
    "    min_logit = None\n",
    "    max_logit = None\n",
    "    sensitivity = None\n",
    "    logits_processor = None\n",
    "\n",
    "    tokenizer = None\n",
    "    model = None\n",
    "    device = None\n",
    "\n",
    "    def __init__(self, model_checkpoint=\"~/google/flan-t5-base\", min_logit=-19.22705113016047, max_logit=7.48324937989716):\n",
    "        self.model_checkpoint = model_checkpoint\n",
    "\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_checkpoint)\n",
    "        self.model = AutoModelForSeq2SeqLM.from_pretrained(self.model_checkpoint).to(self.device)\n",
    "\n",
    "        self.min_logit = min_logit\n",
    "        self.max_logit = max_logit\n",
    "        self.sensitivity = abs(self.max_logit - self.min_logit)\n",
    "        self.logits_processor = LogitsProcessorList([ClipLogitsProcessor(self.min_logit, self.max_logit)])\n",
    "\n",
    "\n",
    "    def prompt_template_fn(self, doc):\n",
    "        prompt = \"Document : {}\\nParaphrase of the document :\".format(doc)\n",
    "        return prompt\n",
    "    \n",
    "    def privatize(self, text, epsilon=100):\n",
    "        temperature = 2 * self.sensitivity / epsilon\n",
    "\n",
    "        prompt = self.prompt_template_fn(text)\n",
    "\n",
    "        model_inputs = self.tokenizer(prompt, max_length=512, truncation=True, return_tensors=\"pt\").to(self.device)\n",
    "        output = self.model.generate(\n",
    "            **model_inputs,\n",
    "            do_sample = True,\n",
    "            top_k=0,\n",
    "            top_p=1.0,\n",
    "            temperature=temperature,\n",
    "            max_new_tokens=len(model_inputs[\"input_ids\"][0]),\n",
    "            logits_processor=self.logits_processor)\n",
    "\n",
    "        private_text = self.tokenizer.decode(output[0], skip_special_tokens = True )\n",
    "        return private_text\n",
    "    \n",
    "class DPParaphrase():\n",
    "    model_checkpoint = None\n",
    "    min_logit = None\n",
    "    max_logit = None\n",
    "    sensitivity = None\n",
    "    logits_processor = None\n",
    "\n",
    "    tokenizer = None\n",
    "    model = None\n",
    "    device = None\n",
    "\n",
    "    def __init__(self, model_checkpoint=(\"models/gpt2-paraphraser\"), min_logit=-96.85249956065758, max_logit=-8.747697966442914):\n",
    "        self.model_checkpoint = model_checkpoint\n",
    "\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_checkpoint)\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(self.model_checkpoint, pad_token_id=self.tokenizer.eos_token_id).to(self.device)\n",
    "\n",
    "        self.min_logit = min_logit\n",
    "        self.max_logit = max_logit\n",
    "        self.sensitivity = abs(self.max_logit - self.min_logit)\n",
    "        self.logits_processor = LogitsProcessorList([ClipLogitsProcessor(self.min_logit, self.max_logit)])\n",
    "\n",
    "        self.pipe = pipeline(task=\"text-generation\", model=self.model, \n",
    "                             tokenizer=self.tokenizer, \n",
    "                             logits_processor=self.logits_processor, \n",
    "                             device=self.device,\n",
    "                             pad_token_id=self.tokenizer.eos_token_id)\n",
    "\n",
    "    def privatize(self, text, epsilon=100):\n",
    "        temperature = 2 * self.sensitivity / epsilon\n",
    "\n",
    "        prompt = text+ \" >>>>> \"\n",
    "        length = len(self.tokenizer(prompt)[\"input_ids\"])\n",
    "    \n",
    "        private_text = self.pipe(prompt, max_new_tokens=length, temperature=temperature)[0][\"generated_text\"]\n",
    "        return private_text.replace(prompt, \"\").replace(prompt.strip(), \"\").replace(u'\\xa0', u' ').replace(\">\", \"\").strip()\n",
    "   \n",
    "class DPBart():\n",
    "    model = None\n",
    "    decoder = None\n",
    "    tokenizer = None\n",
    "\n",
    "    sigma = None\n",
    "    num_sigmas = None\n",
    "    c_min = None\n",
    "    c_max = None\n",
    "    delta = None\n",
    "\n",
    "    def __init__(self, model='facebook/bart-base', num_sigmas=1/2):\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "        self.tokenizer = BartTokenizer.from_pretrained(model)\n",
    "        self.model = BartModel.from_pretrained(model).to(self.device)\n",
    "        self.decoder = BartForConditionalGeneration.from_pretrained(model).to(self.device)\n",
    "\n",
    "        self.delta = 1e-5\n",
    "        self.sigma = 0.2\n",
    "        self.num_sigmas = num_sigmas\n",
    "        self.c_min = -self.sigma\n",
    "        self.c_max = self.num_sigmas * self.sigma\n",
    "\n",
    "    def clip(self, vector):\n",
    "        return torch.clip(vector, self.c_min, self.c_max)\n",
    "\n",
    "    # https://github.com/trusthlt/dp-bart-private-rewriting/\n",
    "    def calibrateAnalyticGaussianMechanism_precision(self, epsilon, delta, GS, tol = 1.e-12):\n",
    "        \"\"\"\n",
    "        High-precision version of `calibrateAnalyticGaussianMechanism`.\n",
    "        Should not be used for epsilon values above 5000.\n",
    "        \"\"\"\n",
    "\n",
    "        if epsilon <= 1000:\n",
    "            mp.dps = 500  # works for epsilon = 1000\n",
    "        elif epsilon <= 2500 and epsilon > 1000:\n",
    "            mp.dps = 1100  # works for epsilon = 2500\n",
    "        else:\n",
    "            mp.dps = 2200  # works for epsilon = 5000\n",
    "\n",
    "        def Phi(t):\n",
    "            return 0.5*(1.0 + mpmath.erf(t/mpmath.sqrt(2.0)))\n",
    "\n",
    "        def caseA(epsilon,s):\n",
    "            return Phi(mpmath.sqrt(epsilon*s)) - mpmath.exp(epsilon)*Phi(-mpmath.sqrt(epsilon*(s+2.0)))\n",
    "\n",
    "        def caseB(epsilon,s):\n",
    "            return Phi(-mpmath.sqrt(epsilon*s)) - mpmath.exp(epsilon)*Phi(-mpmath.sqrt(epsilon*(s+2.0)))\n",
    "\n",
    "        def doubling_trick(predicate_stop, s_inf, s_sup):\n",
    "            while(not predicate_stop(s_sup)):\n",
    "                s_inf = s_sup\n",
    "                s_sup = 2.0*s_inf\n",
    "            return s_inf, s_sup\n",
    "\n",
    "        def binary_search(predicate_stop, predicate_left, s_inf, s_sup):\n",
    "            s_mid = s_inf + (s_sup-s_inf)/2.0\n",
    "            while(not predicate_stop(s_mid)):\n",
    "                if (predicate_left(s_mid)):\n",
    "                    s_sup = s_mid\n",
    "                else:\n",
    "                    s_inf = s_mid\n",
    "                s_mid = s_inf + (s_sup-s_inf)/2.0\n",
    "            return s_mid\n",
    "\n",
    "        delta_thr = caseA(epsilon, 0.0)\n",
    "\n",
    "        if (delta == delta_thr):\n",
    "            alpha = 1.0\n",
    "\n",
    "        else:\n",
    "            if (delta > delta_thr):\n",
    "                predicate_stop_DT = lambda s : caseA(epsilon, s) >= delta\n",
    "                function_s_to_delta = lambda s : caseA(epsilon, s)\n",
    "                predicate_left_BS = lambda s : function_s_to_delta(s) > delta\n",
    "                function_s_to_alpha = lambda s : mpmath.sqrt(1.0 + s/2.0) - mpmath.sqrt(s/2.0)\n",
    "\n",
    "            else:\n",
    "                predicate_stop_DT = lambda s : caseB(epsilon, s) <= delta\n",
    "                function_s_to_delta = lambda s : caseB(epsilon, s)\n",
    "                predicate_left_BS = lambda s : function_s_to_delta(s) < delta\n",
    "                function_s_to_alpha = lambda s : mpmath.sqrt(1.0 + s/2.0) + mpmath.sqrt(s/2.0)\n",
    "\n",
    "            predicate_stop_BS = lambda s : abs(function_s_to_delta(s) - delta) <= tol\n",
    "\n",
    "            s_inf, s_sup = doubling_trick(predicate_stop_DT, 0.0, 1.0)\n",
    "            s_final = binary_search(predicate_stop_BS, predicate_left_BS, s_inf, s_sup)\n",
    "            alpha = function_s_to_alpha(s_final)\n",
    "\n",
    "        sigma = alpha*GS/mpmath.sqrt(2.0*epsilon)\n",
    "\n",
    "        return float(sigma)\n",
    "    \n",
    "    def noise(self, vector, epsilon, delta=1e-5, method=\"gaussian\"):\n",
    "        k = vector.shape[-1]\n",
    "        if method == \"laplace\":\n",
    "            sensitivity = 2 * self.sigma * self.num_sigmas * k\n",
    "            Z = torch.from_numpy(np.random.laplace(0, sensitivity / epsilon, size=k))\n",
    "        elif method == 'gaussian':\n",
    "            sensitivity = 2 * self.sigma * self.num_sigmas * np.sqrt(k)\n",
    "            scale = np.sqrt((sensitivity**2 / epsilon**2) * 2 * np.log(1.25 / self.delta))\n",
    "            Z = torch.from_numpy(np.random.normal(0, scale, size=k))\n",
    "        elif method == \"analytic_gaussian\":\n",
    "            sensitivity = 2 * self.sigma * self.num_sigmas * np.sqrt(k)\n",
    "            analytic_scale = self.calibrateAnalyticGaussianMechanism_precision(epsilon, self.delta, sensitivity)\n",
    "            Z = torch.from_numpy(np.random.normal(0, analytic_scale, size=k))\n",
    "        return vector + Z\n",
    "\n",
    "    def privatize(self, text, epsilon=100, method=\"gaussian\"):\n",
    "        inputs = self.tokenizer(text, max_length=512, truncation=True, return_tensors=\"pt\").to(self.device)\n",
    "        num_tokens = len(inputs[\"input_ids\"][0])\n",
    "\n",
    "        enc_output = self.model.encoder(**inputs)\n",
    "        enc_output[\"last_hidden_state\"] = self.noise(self.clip(enc_output[\"last_hidden_state\"].cpu()), epsilon=epsilon, delta=self.delta, method=method).float().to(self.device)\n",
    "\n",
    "        dec_out = self.decoder.generate(encoder_outputs=enc_output, max_new_tokens=num_tokens)\n",
    "        private_text = self.tokenizer.decode(dec_out[0], skip_special_tokens=True)\n",
    "        return private_text.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b9a65f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional installs (uncomment if needed)\n",
    "# !pip install -q transformers datasets evaluate accelerate\n",
    "\n",
    "import os\n",
    "import evaluate\n",
    "from datasets import load_dataset, DatasetDict, Value\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    DataCollatorWithPadding,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# 0) REQUIRED: your privatizers\n",
    "# -----------------------------\n",
    "# M, N, K must exist and expose:\n",
    "#   M.dpmlm_rewrite(text: str, epsilon: int) -> str\n",
    "#   N.Privatize(text: str, epsilon: int) -> str\n",
    "#   K.privatize(text: str, epsilon: int) -> str\n",
    "#\n",
    "# from your_module import M, N, K\n",
    "# M, N, K = ...\n",
    "# For safety, add NLTK resources if your methods rely on them:\n",
    "try:\n",
    "    import nltk\n",
    "    for res in [\"punkt\", \"punkt_tab\"]:\n",
    "        try: nltk.data.find(f\"tokenizers/{res}\")\n",
    "        except LookupError: nltk.download(res)\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# -----------------------------\n",
    "# 1) Config\n",
    "# -----------------------------\n",
    "MODEL_NAME = \"microsoft/deberta-v3-base\"\n",
    "MAX_LEN = 128\n",
    "BATCH = 16\n",
    "LR = 5e-5\n",
    "EPOCHS = 1\n",
    "WD = 0.01\n",
    "OUTPUT_BASE = \"dv3-priv\"\n",
    "EPSILONS = [10, 25, 50, 100, 250]   # change as you like\n",
    "TECHNIQUES = [\"dpmlm\", \"privN\", \"privK\"]  # M, N, K\n",
    "\n",
    "# For IMDB-Genres local file\n",
    "IMDB_PATH = \"data/imdb_genres.csv\"  # change to your file path\n",
    "IMDB_DELIM = \",\"                    # \",\" for CSV or \"\\t\" for TSV\n",
    "IMDB_TEXT_COL = \"text\"\n",
    "IMDB_LABEL_COL = \"label\"\n",
    "\n",
    "# Map datasets to their input text columns\n",
    "# (name, subset, text1, text2, notes)\n",
    "DATASETS = [\n",
    "    # SNLI (3-class): premise/hypothesis, filter label == -1\n",
    "    (\"snli\", None, \"premise\", \"hypothesis\"),\n",
    "    # QQP (binary)\n",
    "    (\"glue\", \"qqp\", \"question1\", \"question2\"),\n",
    "    # PAWS labeled_final (binary)\n",
    "    (\"paws\", \"labeled_final\", \"sentence1\", \"sentence2\"),\n",
    "    # IMDB-Genres (multi-class) — local file, single text column\n",
    "    (\"imdb_genres_local\", None, IMDB_TEXT_COL, None),\n",
    "]\n",
    "\n",
    "# -----------------------------\n",
    "# 2) Utilities\n",
    "# -----------------------------\n",
    "def make_training_args(**base_kwargs):\n",
    "    \"\"\"Shim: transformers v5 uses eval_strategy, v4 uses evaluation_strategy.\"\"\"\n",
    "    try:\n",
    "        return TrainingArguments(eval_strategy=\"epoch\", **base_kwargs)\n",
    "    except TypeError:\n",
    "        return TrainingArguments(evaluation_strategy=\"epoch\", **base_kwargs)\n",
    "\n",
    "def _to_str(x):\n",
    "    if x is None: return \"\"\n",
    "    return x if isinstance(x, str) else str(x)\n",
    "\n",
    "def privatize_fn_builder(tech: str, eps: int, col1: str, col2: str | None):\n",
    "    \"\"\"Return a batched map function that rewrites text columns.\"\"\"\n",
    "    def fn(batch):\n",
    "        out = {}\n",
    "        # primary column\n",
    "        if tech == \"dpmlm\":\n",
    "            out[col1] = [ _to_str(M.dpmlm_rewrite(_to_str(t), eps)) for t in batch[col1] ]\n",
    "        elif tech == \"privN\":\n",
    "            out[col1] = [ _to_str(N.Privatize(_to_str(t), eps))      for t in batch[col1] ]\n",
    "        elif tech == \"privK\":\n",
    "            out[col1] = [ _to_str(K.privatize(_to_str(t), eps))      for t in batch[col1] ]\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown technique: {tech}\")\n",
    "\n",
    "        # secondary column (if any)\n",
    "        if col2 is not None:\n",
    "            if tech == \"dpmlm\":\n",
    "                out[col2] = [ _to_str(M.dpmlm_rewrite(_to_str(t), eps)) for t in batch[col2] ]\n",
    "            elif tech == \"privN\":\n",
    "                out[col2] = [ _to_str(N.Privatize(_to_str(t), eps))      for t in batch[col2] ]\n",
    "            elif tech == \"privK\":\n",
    "                out[col2] = [ _to_str(K.privatize(_to_str(t), eps))      for t in batch[col2] ]\n",
    "\n",
    "        # keep label untouched if present\n",
    "        if \"label\" in batch:\n",
    "            out[\"label\"] = batch[\"label\"]\n",
    "        return out\n",
    "    return fn\n",
    "\n",
    "def tokenize_builder(tok, col1, col2, is_regression=False):\n",
    "    \"\"\"Tokenize and attach labels -> 'labels' field.\"\"\"\n",
    "    def fn(batch):\n",
    "        if col2 is None:\n",
    "            enc = tok(batch[col1], truncation=True, max_length=MAX_LEN)\n",
    "        else:\n",
    "            enc = tok(batch[col1], batch[col2], truncation=True, max_length=MAX_LEN)\n",
    "        if \"label\" in batch:\n",
    "            enc[\"labels\"] = [float(x) for x in batch[\"label\"]] if is_regression else batch[\"label\"]\n",
    "        return enc\n",
    "    return fn\n",
    "\n",
    "def compute_accuracy_builder(num_labels: int):\n",
    "    \"\"\"Classification accuracy metric.\"\"\"\n",
    "    acc = evaluate.load(\"accuracy\")\n",
    "    def compute_metrics(eval_pred):\n",
    "        logits, labels = eval_pred\n",
    "        preds = logits.argmax(axis=-1)\n",
    "        return acc.compute(predictions=preds, references=labels)\n",
    "    return compute_metrics\n",
    "\n",
    "# -----------------------------\n",
    "# 3) Dataset loaders (standardize to train/validation splits)\n",
    "# -----------------------------\n",
    "def load_snli():\n",
    "    ds = load_dataset(\"snli\")\n",
    "    # Filter invalid labels (-1)\n",
    "    ds = ds.filter(lambda ex: ex[\"label\"] != -1)\n",
    "    # SNLI has 'train', 'validation', 'test'\n",
    "    return DatasetDict({\n",
    "        \"train\": ds[\"train\"],\n",
    "        \"validation\": ds[\"validation\"]\n",
    "    })\n",
    "\n",
    "def load_qqp():\n",
    "    ds = load_dataset(\"glue\", \"qqp\")\n",
    "    return DatasetDict({\n",
    "        \"train\": ds[\"train\"],\n",
    "        \"validation\": ds[\"validation\"]\n",
    "    })\n",
    "\n",
    "def load_paws_final():\n",
    "    ds = load_dataset(\"paws\", \"labeled_final\")\n",
    "    return DatasetDict({\n",
    "        \"train\": ds[\"train\"],\n",
    "        \"validation\": ds[\"validation\"]\n",
    "    })\n",
    "\n",
    "def load_imdb_genres_local(path: str, delim: str):\n",
    "    if not os.path.exists(path):\n",
    "        raise FileNotFoundError(f\"IMDB-Genres file not found at: {path}\")\n",
    "    ds = load_dataset(\"csv\", data_files={\"train\": path, \"validation\": path}, delimiter=delim)\n",
    "    # If you actually have a separate validation file, change data_files accordingly.\n",
    "    return ds\n",
    "\n",
    "def get_loader(name, subset):\n",
    "    if name == \"snli\": return load_snli()\n",
    "    if name == \"glue\" and subset == \"qqp\": return load_qqp()\n",
    "    if name == \"paws\" and subset == \"labeled_final\": return load_paws_final()\n",
    "    if name == \"imdb_genres_local\": return load_imdb_genres_local(IMDB_PATH, IMDB_DELIM)\n",
    "    raise ValueError(f\"Unsupported dataset: {name}/{subset}\")\n",
    "\n",
    "# -----------------------------\n",
    "# 4) Main: loop datasets × techniques × epsilons\n",
    "# -----------------------------\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
    "data_collator = DataCollatorWithPadding(tokenizer)\n",
    "\n",
    "for (ds_name, ds_subset, s1, s2) in DATASETS:\n",
    "    print(f\"\\n================ Dataset: {ds_name}{'/' + ds_subset if ds_subset else ''} =================\\n\")\n",
    "    ds = get_loader(ds_name, ds_subset)\n",
    "\n",
    "    # Arrow schema safety — force text columns to string\n",
    "    if s1 in ds[\"train\"].column_names:\n",
    "        ds = ds.cast_column(s1, Value(\"string\"))\n",
    "    if s2 and s2 in ds[\"train\"].column_names:\n",
    "        ds = ds.cast_column(s2, Value(\"string\"))\n",
    "\n",
    "    # Determine number of labels (classification). Assume 'label' exists and is categorical/int.\n",
    "    # For local IMDB, ensure labels are ints (0..C-1).\n",
    "    if \"label\" not in ds[\"train\"].column_names:\n",
    "        raise ValueError(f\"'label' column missing in {ds_name}. Please add/rename accordingly.\")\n",
    "    label_feat = ds[\"train\"].features[\"label\"]\n",
    "    if hasattr(label_feat, \"names\") and label_feat.names:\n",
    "        num_labels = len(label_feat.names)\n",
    "    else:\n",
    "        # Fallback: infer from data if no names (e.g., custom IMDB)\n",
    "        # You can replace with your known num_classes\n",
    "        unique_labels = set(ds[\"train\"][IMDB_LABEL_COL] if ds_name==\"imdb_genres_local\" else ds[\"train\"][\"label\"])\n",
    "        num_labels = len(unique_labels)\n",
    "\n",
    "    # Build model fresh for each run (dataset/tech/epsilon)\n",
    "    for tech in TECHNIQUES:\n",
    "        for eps in EPSILONS:\n",
    "            tag = f\"{ds_name}{('-' + ds_subset) if ds_subset else ''}-{tech}-eps{eps}\"\n",
    "            print(f\"\\n---- {tag} ----\")\n",
    "\n",
    "            # 1) Privatize\n",
    "            priv_fn = privatize_fn_builder(tech, eps, s1, s2)\n",
    "            ds_priv = DatasetDict({\n",
    "                \"train\": ds[\"train\"].map(priv_fn, batched=True),\n",
    "                \"validation\": ds[\"validation\"].map(priv_fn, batched=True),\n",
    "            })\n",
    "\n",
    "            # 2) Tokenize and attach labels\n",
    "            tok_fn = tokenize_builder(tokenizer, s1, s2, is_regression=False)\n",
    "            remove_cols = [c for c in ds_priv[\"train\"].column_names if c not in {s1, s2, \"label\"}]\n",
    "            enc = DatasetDict({\n",
    "                \"train\": ds_priv[\"train\"].map(tok_fn, batched=True, remove_columns=remove_cols),\n",
    "                \"validation\": ds_priv[\"validation\"].map(tok_fn, batched=True, remove_columns=remove_cols),\n",
    "            })\n",
    "\n",
    "            # 3) Model + Trainer\n",
    "            model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=num_labels)\n",
    "\n",
    "            args = make_training_args(\n",
    "                output_dir=f\"{OUTPUT_BASE}-{tag}\",\n",
    "                save_strategy=\"epoch\",\n",
    "                learning_rate=LR,\n",
    "                per_device_train_batch_size=BATCH,\n",
    "                per_device_eval_batch_size=BATCH,\n",
    "                num_train_epochs=EPOCHS,\n",
    "                weight_decay=WD,\n",
    "                logging_dir=f\"{OUTPUT_BASE}-{tag}/logs\",\n",
    "                report_to=\"none\",\n",
    "                load_best_model_at_end=False,\n",
    "                logging_steps=50,\n",
    "                # fp16=True,  # uncomment if your GPU supports it\n",
    "            )\n",
    "\n",
    "            trainer = Trainer(\n",
    "                model=model,\n",
    "                args=args,\n",
    "                train_dataset=enc[\"train\"],\n",
    "                eval_dataset=enc[\"validation\"],\n",
    "                tokenizer=tokenizer,\n",
    "                data_collator=data_collator,\n",
    "                compute_metrics=compute_accuracy_builder(num_labels),\n",
    "            )\n",
    "\n",
    "            # 4) Train + Evaluate\n",
    "            trainer.train()\n",
    "            metrics = trainer.evaluate()\n",
    "            print(f\"RESULT [{tag}] -> {metrics}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
