{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2036034c",
   "metadata": {},
   "source": [
    "## code to privatise glue dataset using dp-mlm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e360940",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import nltk\n",
    "import json\n",
    "import string\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import wn\n",
    "nltk.download('words', quiet=True)\n",
    "nltk.download(\"punkt\", quiet=True)\n",
    "nltk.download(\"stopwords\", quiet=True)\n",
    "nltk.download(\"words\", quiet=True)\n",
    "nltk.download('averaged_perceptron_tagger', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from transformers import AutoModel, AutoTokenizer, AutoModelForMaskedLM, logging\n",
    "import importlib_resources as impresources\n",
    "\n",
    "wn.download('oewn:2022')\n",
    "en = wn.Wordnet('oewn:2022')\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "logging.set_verbosity_warning()\n",
    "\n",
    "stop = set([x for x in stopwords.words(\"english\")])\n",
    "\n",
    "def nth_repl(s, sub, repl, n):\n",
    "    s_split = s.split()\n",
    "    i = 0\n",
    "    try:\n",
    "        find = s_split.index(sub)\n",
    "        i += 1\n",
    "    except ValueError:\n",
    "        return s\n",
    "\n",
    "    while i != n:\n",
    "        try:\n",
    "            find = s_split.index(sub, find + 1)\n",
    "            i += 1\n",
    "        except ValueError:\n",
    "            break\n",
    "    if i == n:\n",
    "        return \" \".join(s_split[:find] + [repl] + s_split[find+1:])\n",
    "    return s\n",
    "\n",
    "def nth_rem(s, sub, n):\n",
    "    s_split = s.split()\n",
    "    i = 0\n",
    "    try:\n",
    "        find = s_split.index(sub)\n",
    "        i += 1\n",
    "    except ValueError:\n",
    "        return s\n",
    "\n",
    "    while i != n:\n",
    "        try:\n",
    "            find = s_split.index(sub, find + 1)\n",
    "            i += 1\n",
    "        except ValueError:\n",
    "            break\n",
    "    if i == n:\n",
    "        return \" \".join(s_split[:find] + s_split[find+1:])\n",
    "    return s\n",
    "\n",
    "def sentence_enum(tokens):\n",
    "    counts = Counter()\n",
    "    n = []\n",
    "    for t in tokens:\n",
    "        counts[t] += 1\n",
    "        n.append(counts[t])\n",
    "    return n\n",
    "\n",
    "def get_opposites():\n",
    "\twith open(\"/content/drive/MyDrive/dpmlm_data/opposites.json\", 'r') as f:\n",
    "\t\topposites = json.load(f)\n",
    "\treturn opposites\n",
    "\n",
    "def get_vocab():\n",
    "\twith open(\"/content/drive/MyDrive/dpmlm_data/vocab.txt\", 'r',encoding=\"utf-8\") as f:\n",
    "\t\tvocab = set([x.strip() for x in f.readlines()])\n",
    "\treturn vocab\n",
    "\n",
    "def get_antonyms(word):\n",
    "    ants = list()\n",
    "\n",
    "    #Get antonyms from WordNet for this word and any of its synonyms.\n",
    "    for ss in en.synsets(word):\n",
    "        for sense in ss.senses():\n",
    "            ants.extend([x.word().lemma() for x in sense.get_related(\"antonym\")])\n",
    "\n",
    "    #Get snyonyms of antonyms found in the previous step, thus expanding the list even more.\n",
    "    syns = list()\n",
    "    for word in list(set(ants)):\n",
    "        for ss in en.synsets(word):\n",
    "            syns.extend(ss.lemmas())\n",
    "\n",
    "    return sorted(list(set(syns)))\n",
    "\n",
    "'''\n",
    "Gets pertainyms of the target word from the WordNet knowledge base.\n",
    "* pertainyms = words pertaining to the target word (industrial -> pertainym is \"industry\")\n",
    "'''\n",
    "def get_pertainyms(word):\n",
    "    perts = list()\n",
    "    for ss in en.synsets(word):\n",
    "        for sense in ss.senses():\n",
    "            perts.extend([x.word().lemma() for x in sense.get_related(\"pertainym\")])\n",
    "    return sorted(list(set(perts)))\n",
    "'''\n",
    "Get hyponyms (new wn)\n",
    "'''\n",
    "def get_hyponyms(word):\n",
    "    hypo = list()\n",
    "    for ss in en.synsets(word):\n",
    "        for sense in ss.senses():\n",
    "            hypo.extend([x.word().lemma() for x in sense.get_related(\"hyponyms\")])\n",
    "    return sorted(list(set(hypo)))\n",
    "\n",
    "'''\n",
    "Get hypernyms (new wn)\n",
    "'''\n",
    "def get_hypernyms(word):\n",
    "    hyper = list()\n",
    "    for ss in en.synsets(word):\n",
    "        for h in ss.hypernyms():\n",
    "            hyper.extend([x.lemma() for x in h.words()])\n",
    "    return sorted(list(set(hyper)))\n",
    "\n",
    "'''\n",
    "Gets derivationally related forms (e.g. begin -> 'beginner', 'beginning')\n",
    "'''\n",
    "def get_related_forms(word):\n",
    "    forms = list()\n",
    "    for ss in wn.synsets(word):\n",
    "        for sense in ss.senses():\n",
    "            forms.extend([x.word().lemma() for x in sense.get_related(\"derivation\")])\n",
    "    return sorted(list(set(forms)))\n",
    "\n",
    "'''\n",
    "General get nym\n",
    "'''\n",
    "def get_general_nym(word, nym):\n",
    "    n = list()\n",
    "    for ss in wn.synsets(word):\n",
    "        for sense in ss.senses():\n",
    "            n.extend([x.word().lemma() for x in sense.get_related(nym)])\n",
    "    return sorted(list(set(n)))\n",
    "\n",
    "'''\n",
    "Gets antonyms, hypernyms, hyponyms, holonyms, meronyms, pertainyms, and derivationally related forms of a target word from WordNet.\n",
    "* hypernym = a word whose meaning includes a group of other words (\"animal\" is a hypernym of \"dog\")\n",
    "* hyponym = a word whose meaning is included in the meaning of another word (\"bulldog\" is a hyponym of \"dog\")\n",
    "* a meronym denotes a part and a holonym denotes a whole: \"week\" is a holonym of \"weekend\", \"eye\" is a meronym of \"face\", and vice-versa\n",
    "'''\n",
    "def get_nyms(word, depth=-1):\n",
    "    nym_list = ['antonyms', 'hypernyms', 'hyponyms', 'holonyms', 'meronyms',\n",
    "                'pertainyms', 'derivationally_related_forms']\n",
    "    results = list()\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    word = lemmatizer.lemmatize(word)\n",
    "\n",
    "    def query_wordnet(getter):\n",
    "        res = list()\n",
    "        for ss in en.synsets(word):\n",
    "            res_list = [item.lemmas() for item in ss.closure(getter)]\n",
    "            res_list = [item.name() for sublist in res_list for item in sublist]\n",
    "            res.extend(res_list)\n",
    "        return res\n",
    "\n",
    "    for nym in nym_list:\n",
    "        if nym=='antonyms':\n",
    "            results.append(get_antonyms(word))\n",
    "\n",
    "        elif nym == \"hypernyms\":\n",
    "            results.append(get_hypernyms(word))\n",
    "\n",
    "        elif nym == \"hyponyms\":\n",
    "            results.append(get_hyponyms(word))\n",
    "\n",
    "        elif nym in ['holonyms', 'meronyms']:\n",
    "            res = list()\n",
    "            #Three different types of holonyms and meronyms as defined in WordNet\n",
    "            for postfix in [\"_member\", \"_part\", \"_portion\", \"_substance\"]:\n",
    "                res.extend(get_general_nym(word, \"{}{}\".format(nym[:4], postfix)))\n",
    "            results.append(res)\n",
    "\n",
    "        elif nym=='pertainyms':\n",
    "            results.append(get_pertainyms(word))\n",
    "\n",
    "        else:\n",
    "            results.append(get_related_forms(word))\n",
    "\n",
    "    results = map(set, results)\n",
    "    nyms = dict(zip(nym_list, results))\n",
    "    return nyms\n",
    "\n",
    "#Converts a part-of-speech tag returned by NLTK to a POS tag from WordNet\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wn.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wn.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wn.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wn.ADV\n",
    "    else:\n",
    "        return ''\n",
    "\n",
    "#Function for clearing up duplicate words (capitalized, upper-case, etc.), stop words, and antonyms from the list of candidates.\n",
    "def filter_words(target, words, scr, tkn, opp={}):\n",
    "    dels = list()\n",
    "    toks = tkn.tolist()\n",
    "    nyms = get_nyms(target)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    if lemmatizer.lemmatize(target.lower()) in opp:\n",
    "        opp_del = [x for x in words if lemmatizer.lemmatize(x.lower()) in opp[lemmatizer.lemmatize(target.lower())]]\n",
    "        dels.extend(opp_del)\n",
    "\n",
    "    for w in words:\n",
    "        if w.lower() in words and w.upper() in words:\n",
    "            dels.append(w.upper())\n",
    "        if lemmatizer.lemmatize(w.lower()) in nyms['antonyms']:\n",
    "            dels.append(w)\n",
    "\n",
    "    dels = list(set(dels))\n",
    "    for d in dels:\n",
    "        del scr[words.index(d)]\n",
    "        del toks[words.index(d)]\n",
    "        words.remove(d)\n",
    "\n",
    "    return words, scr, torch.tensor(toks)\n",
    "\n",
    "#Calculates the similarity score\n",
    "def similarity_score(original_output, subst_output, k):\n",
    "    mask_idx = k\n",
    "    cos_sim = torch.nn.CosineSimilarity(dim=1, eps=1e-6)\n",
    "    weights = torch.div(torch.stack(list(original_output[3])).squeeze().sum(0).sum(0), (12 * 12.0))\n",
    "\n",
    "    suma = 0.0\n",
    "    sent_len = original_output[2][2].shape[1]\n",
    "\n",
    "    for token_idx in range(sent_len):\n",
    "        original_hidden = original_output[2]\n",
    "        subst_hidden = subst_output[2]\n",
    "\n",
    "        #Calculate the contextualized representation of the i-th word as a concatenation of RoBERTa's values in its last four layers\n",
    "        context_original = torch.cat( tuple( [original_hidden[hs_idx][:, token_idx, :] for hs_idx in [1, 2, 3, 4]] ), dim=1)\n",
    "        context_subst = torch.cat( tuple( [subst_hidden[hs_idx][:, token_idx, :] for hs_idx in [1, 2, 3, 4]] ), dim=1)\n",
    "        suma += weights[mask_idx][token_idx] * cos_sim(context_original, context_subst)\n",
    "\n",
    "    substitute_validation = suma\n",
    "    return substitute_validation\n",
    "\n",
    "#Calculates the proposal score\n",
    "def proposal_score(original_score, subst_scores, device):\n",
    "    subst_scores = torch.tensor(subst_scores).to(device)\n",
    "    return np.log( torch.div(subst_scores , (1.0 - original_score)).cpu() )\n",
    "\n",
    "class DPMLM():\n",
    "    opposites = get_opposites()\n",
    "    vocab = get_vocab()\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    detokenizer = TreebankWordDetokenizer()\n",
    "    tokenizer = None\n",
    "    lm_model = None\n",
    "    raw_model = None\n",
    "    device = None\n",
    "    nlp = None\n",
    "    alpha = None\n",
    "\n",
    "    def __init__(self, MODEL=\"roberta-base\", SPACY=\"en_core_web_md\", alpha=0.003):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "        self.lm_model = AutoModelForMaskedLM.from_pretrained(MODEL)\n",
    "        self.raw_model = AutoModel.from_pretrained(MODEL, output_hidden_states=True, output_attentions=True)\n",
    "        self.alpha = alpha\n",
    "\n",
    "        self.clip_min = -3.2093127\n",
    "        self.clip_max = 16.304797887802124\n",
    "        self.sensitivity = abs(self.clip_max - self.clip_min)\n",
    "\n",
    "        self.device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.lm_model = self.lm_model.to(self.device)\n",
    "        self.raw_model = self.raw_model.to(self.device)\n",
    "\n",
    "    def load_transformers(self):\n",
    "        return self.tokenizer, self.lm_model, self.raw_model\n",
    "\n",
    "    #Calculates the proposal scores, substitute validation scores, and then the final score for each candidate word's fit as a substitution.\n",
    "    def calc_scores(self, scr, sentences, original_output, original_score, mask_index):\n",
    "        #Get representations of all substitute sentences\n",
    "        _, _, raw_model = self.load_transformers()\n",
    "        subst_output = raw_model(sentences)\n",
    "\n",
    "        prop_score = proposal_score(original_score, scr, self.device)\n",
    "        substitute_validation = similarity_score(original_output, subst_output, mask_index)\n",
    "\n",
    "        final_score = substitute_validation.cpu() + self.alpha*prop_score\n",
    "\n",
    "        return final_score, prop_score, substitute_validation\n",
    "\n",
    "    def privatize(self, sentence, target, n=1, K=5, CONCAT=True, FILTER=True, POS=False, ENGLISH=False, epsilon=1, MS=None, TEMP=False):\n",
    "        split_sent = nltk.word_tokenize(sentence)\n",
    "        original_sent = ' '.join(split_sent)\n",
    "        #orig_pos = [x.tag_ for x in self.nlp(original_sent)]\n",
    "\n",
    "        # Masks the target word in the original sentence.\n",
    "        if MS is None:\n",
    "            masked_sent = ' '.join(split_sent)\n",
    "        else:\n",
    "            masked_sent = MS\n",
    "\n",
    "        if isinstance(target, list):\n",
    "            if n == 1:\n",
    "                n = [1 for _ in range(len(target))]\n",
    "\n",
    "            for t, nn in zip(target, n):\n",
    "                masked_sent = nth_repl(masked_sent, t, self.tokenizer.mask_token, nn)\n",
    "        else:\n",
    "            masked_sent = nth_repl(masked_sent, target, self.tokenizer.mask_token, n)\n",
    "            n = [n]\n",
    "\n",
    "        #Get the input token IDs of the input consisting of: the original sentence + separator + the masked sentence.\n",
    "        if CONCAT == False:\n",
    "            input_ids = self.tokenizer.encode(\" \"+masked_sent, add_special_tokens=True)\n",
    "        else:\n",
    "            input_ids = self.tokenizer.encode(\" \"+original_sent.replace(\"MASK\", \"\"), \" \"+masked_sent, add_special_tokens=True)\n",
    "        if isinstance(target, list):\n",
    "            masked_position = np.where(np.array(input_ids) == self.tokenizer.mask_token_id)[0].tolist()\n",
    "        else:\n",
    "            masked_position = [input_ids.index(self.tokenizer.mask_token_id)]\n",
    "            target = [target]\n",
    "\n",
    "        original_output = self.raw_model(torch.tensor(input_ids).reshape(1, len(input_ids)).to(self.device))\n",
    "\n",
    "        #Get the predictions of the Masked LM transformer.\n",
    "        with torch.no_grad():\n",
    "            output = self.lm_model(torch.tensor(input_ids).reshape(1, len(input_ids)).to(self.device))\n",
    "\n",
    "        logits = output[0].squeeze().detach().cpu().numpy()\n",
    "\n",
    "        predictions = {}\n",
    "        for t, m, nn in zip(target, masked_position, n):\n",
    "            current = \"{}_{}\".format(t, nn)\n",
    "\n",
    "            #Get top guesses: their token IDs, scores, and words.\n",
    "            mask_logits = logits[m].squeeze()\n",
    "            if TEMP == True:\n",
    "                mask_logits = np.clip(mask_logits, self.clip_min, self.clip_max)\n",
    "                mask_logits = mask_logits / (2 * self.sensitivity / epsilon)\n",
    "\n",
    "                logits_idx = [i for i, x in enumerate(mask_logits)]\n",
    "                scores = torch.softmax(torch.from_numpy(mask_logits), dim=0)\n",
    "                scores = scores / scores.sum()\n",
    "                chosen_idx = np.random.choice(logits_idx, p=scores.numpy())\n",
    "                predictions[current] = (self.tokenizer.decode(chosen_idx).strip(), scores[chosen_idx])\n",
    "                continue\n",
    "            else:\n",
    "                top_tokens = torch.topk(torch.from_numpy(mask_logits), k=K, dim=0)[1]\n",
    "                scores = torch.softmax(torch.from_numpy(mask_logits), dim=0)[top_tokens].tolist()\n",
    "            words = [self.tokenizer.decode(i.item()).strip() for i in top_tokens]\n",
    "\n",
    "            if FILTER == True:\n",
    "                words, scores, top_tokens = filter_words(t, words, scores, top_tokens, self.opposites)\n",
    "\n",
    "            if len(words) == 0:\n",
    "                predictions[current] = [(t, 1)]\n",
    "                continue\n",
    "\n",
    "\n",
    "            assert len(words) == len(scores)\n",
    "\n",
    "            if len(words) == 0:\n",
    "                predictions[current] = [(t, 1)]\n",
    "                continue\n",
    "\n",
    "            original_score = torch.softmax(torch.from_numpy(mask_logits), dim=0)[m]\n",
    "            sentences = list()\n",
    "\n",
    "            for i in range(len(words)):\n",
    "                subst_word = top_tokens[i]\n",
    "                input_ids[m] = int(subst_word)\n",
    "                sentences.append(list(input_ids))\n",
    "\n",
    "            torch_sentences = torch.tensor(sentences).to(self.device)\n",
    "\n",
    "            finals, _, _ = self.calc_scores(scores, torch_sentences, original_output, original_score, m)\n",
    "            finals = map(lambda f : float(f), finals)\n",
    "\n",
    "            zipped = dict(zip(words, finals))\n",
    "            for i in range(len(words)):\n",
    "                cand = words[i]\n",
    "                if cand not in zipped:\n",
    "                    continue\n",
    "\n",
    "                # remove non-words\n",
    "                if ENGLISH == True:\n",
    "                    if cand not in self.vocab and self.lemmatizer.lemmatize(cand) not in self.vocab:\n",
    "                        del zipped[cand]\n",
    "                        continue\n",
    "\n",
    "            zipped = dict(zipped)\n",
    "            finish = list(sorted(zipped.items(), key=lambda item: item[1], reverse=True))[:K]\n",
    "            predictions[current] = finish\n",
    "\n",
    "        if TEMP == True:\n",
    "            for p in predictions:\n",
    "                predictions[p] = predictions[p][0]\n",
    "\n",
    "        return predictions\n",
    "\n",
    "    def dpmlm_rewrite(self, sentence, epsilon, REPLACE=False, FILTER=False, STOP=False, TEMP=True, POS=True, CONCAT=True):\n",
    "        if isinstance(sentence, list):\n",
    "            tokens = sentence\n",
    "        else:\n",
    "            tokens = nltk.word_tokenize(sentence)\n",
    "\n",
    "        if isinstance(epsilon, list):\n",
    "            word_eps = epsilon\n",
    "        else:\n",
    "            word_eps = [epsilon for _ in range(len(tokens))] #epsilon #/ num_tokens\n",
    "        n = sentence_enum(tokens)\n",
    "        replace = []\n",
    "        new_tokens = [str(x) for x in tokens]\n",
    "\n",
    "        perturbed = 0\n",
    "        total = 0\n",
    "        for i, (t, nn, eps) in enumerate(zip(tokens, n, word_eps)):\n",
    "            if i >= len(tokens):\n",
    "                break\n",
    "\n",
    "            if (STOP == False and t in stop) or t in string.punctuation:\n",
    "                total += 1\n",
    "                if tokens[i][0].isupper() == True:\n",
    "                    replace.append(t.capitalize())\n",
    "                else:\n",
    "                    replace.append(t)\n",
    "                continue\n",
    "\n",
    "            if REPLACE == True:\n",
    "                new_s = \" \".join(new_tokens)\n",
    "                new_n = sentence_enum(new_tokens)\n",
    "                res = self.privatize(sentence, t, n=new_n[i], ENGLISH=True, FILTER=FILTER, epsilon=eps, MS=new_s, TEMP=TEMP, POS=POS, CONCAT=CONCAT)\n",
    "                r = res[t+\"_{}\".format(new_n[i])]\n",
    "                new_tokens[i] = r\n",
    "            else:\n",
    "                res = self.privatize(sentence, t, n=nn, ENGLISH=True, FILTER=FILTER, epsilon=eps, TEMP=TEMP, POS=POS, CONCAT=CONCAT)\n",
    "                r = res[t+\"_{}\".format(nn)]\n",
    "\n",
    "            if tokens[i][0].isupper() == True:\n",
    "                replace.append(r.capitalize())\n",
    "            else:\n",
    "                replace.append(r.lower())\n",
    "\n",
    "            if r != t:\n",
    "                perturbed += 1\n",
    "            total += 1\n",
    "\n",
    "        return self.detokenizer.detokenize(replace), perturbed, total\n",
    "\n",
    "    def dpmlm_rewrite_plus(self, sentence, epsilon, FILTER=False, TEMP=True, POS=True, CONCAT=True, ADD_PROB=0.15, DEL_PROB=0.05):\n",
    "        if isinstance(sentence, list):\n",
    "            tokens = sentence\n",
    "        else:\n",
    "            tokens = nltk.word_tokenize(sentence)\n",
    "\n",
    "        if isinstance(epsilon, list):\n",
    "            word_eps = epsilon\n",
    "        else:\n",
    "            word_eps = [epsilon for _ in range(len(tokens))] #epsilon #/ num_tokens\n",
    "        n = sentence_enum(tokens)\n",
    "        replace = []\n",
    "        new_tokens = [str(x) for x in tokens]\n",
    "\n",
    "        perturbed = 0\n",
    "        total = 0\n",
    "        deleted = 0\n",
    "        added = 0\n",
    "\n",
    "        for i, (t, nn, eps) in enumerate(zip(tokens, n, word_eps)):\n",
    "            if t in string.punctuation:\n",
    "                total += 1\n",
    "                replace.append(t)\n",
    "                continue\n",
    "\n",
    "            if i == len(tokens) - 1:\n",
    "                DELETE = 1\n",
    "            else:\n",
    "                DELETE = np.random.rand()\n",
    "            if DELETE >= DEL_PROB:\n",
    "                new_s = \" \".join(new_tokens)\n",
    "                new_n = sentence_enum(new_tokens)\n",
    "                res = self.privatize(sentence, t, n=new_n[i+added-deleted], ENGLISH=True, FILTER=FILTER, epsilon=eps, MS=new_s, TEMP=TEMP, POS=POS, CONCAT=CONCAT)\n",
    "                r = res[t+\"_{}\".format(new_n[i+added-deleted])]\n",
    "                if i+added-deleted > len(new_tokens) - 1:\n",
    "                    new_tokens.insert(i+added-deleted, r)\n",
    "                else:\n",
    "                    new_tokens[i+added-deleted] = r\n",
    "                replace.append(r)\n",
    "\n",
    "                if r != t:\n",
    "                    perturbed += 1\n",
    "                total += 1\n",
    "            else:\n",
    "                new_n = sentence_enum(new_tokens)\n",
    "                temp = nth_rem(\" \".join(new_tokens), t, new_n[i+added-deleted])\n",
    "                new_tokens = [str(x) for x in temp.split()]\n",
    "                deleted += 1\n",
    "                continue\n",
    "\n",
    "            ADD = np.random.rand()\n",
    "            if ADD <= ADD_PROB:\n",
    "                tokens_copy = new_tokens.copy()\n",
    "                tokens_copy.insert(i+1+added-deleted, \"MASK\")\n",
    "                new_s = \" \".join(tokens_copy)\n",
    "                new_n = sentence_enum(new_tokens)\n",
    "                res = self.privatize(sentence, \"MASK\", n=1, ENGLISH=True, FILTER=FILTER, epsilon=eps, MS=new_s, TEMP=TEMP, POS=POS, CONCAT=CONCAT)\n",
    "                r = res[\"MASK_1\"]\n",
    "                new_tokens.insert(i+1+added-deleted, r)\n",
    "                replace.append(r)\n",
    "                added += 1\n",
    "\n",
    "        return self.detokenizer.detokenize(replace), perturbed, total, added, deleted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "887f1e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional installs (uncomment if needed)\n",
    "# !pip install -q transformers datasets evaluate accelerate\n",
    "\n",
    "import json\n",
    "import evaluate\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    DataCollatorWithPadding,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    ")\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 0) Your DP-MLM instance with the required API\n",
    "#    Must define: M.dpmlm_rewrite(text: str, epsilon: int) -> str\n",
    "# -------------------------------------------------\n",
    "# from dpmlm import DPMLM\n",
    "# M = DPMLM(...)\n",
    "M = DPMLM()  # <-- use your real instance\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 1) Config\n",
    "# -------------------------------------------------\n",
    "MODEL_NAME = \"microsoft/deberta-v3-base\"\n",
    "MAX_LENGTH = 128\n",
    "BATCH_SIZE = 16\n",
    "LR = 5e-5\n",
    "EPOCHS = 1\n",
    "WEIGHT_DECAY = 0.01\n",
    "OUTPUT_BASE = \"deberta-v3-glue-dpmlm\"\n",
    "EPSILONS = [10, 25, 50, 100, 250]\n",
    "\n",
    "# GLUE tasks and their input columns\n",
    "TASK_TO_KEYS = {\n",
    "    \"cola\": (\"sentence\", None),\n",
    "    \"sst2\": (\"sentence\", None),\n",
    "    \"mrpc\": (\"sentence1\", \"sentence2\"),\n",
    "    \"qqp\":  (\"question1\", \"question2\"),\n",
    "    \"stsb\": (\"sentence1\", \"sentence2\"),  # regression\n",
    "    \"mnli\": (\"premise\", \"hypothesis\"),\n",
    "    \"qnli\": (\"question\", \"sentence\"),\n",
    "    \"rte\":  (\"sentence1\", \"sentence2\"),\n",
    "    \"wnli\": (\"sentence1\", \"sentence2\"),\n",
    "}\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 2) Helpers (version-safe args + metrics + mapping)\n",
    "# -------------------------------------------------\n",
    "def make_training_args(**base_kwargs):\n",
    "    \"\"\"Shim to work on Transformers v4.x and v5.x.\"\"\"\n",
    "    try:  # Transformers >=5 uses eval_strategy\n",
    "        return TrainingArguments(eval_strategy=\"epoch\", **base_kwargs)\n",
    "    except TypeError:  # Transformers 4.x uses evaluation_strategy\n",
    "        return TrainingArguments(evaluation_strategy=\"epoch\", **base_kwargs)\n",
    "\n",
    "def build_compute_metrics(task_name):\n",
    "    \"\"\"\n",
    "    Always compute ACCURACY for classification tasks.\n",
    "    For STS-B (regression), compute Pearson & Spearman.\n",
    "    \"\"\"\n",
    "    is_reg = (task_name == \"stsb\")\n",
    "    if is_reg:\n",
    "        metric = evaluate.load(\"glue\", \"stsb\")\n",
    "        def compute_metrics(eval_pred):\n",
    "            preds, labels = eval_pred\n",
    "            preds = preds.squeeze()\n",
    "            out = metric.compute(predictions=preds, references=labels)\n",
    "            return {\"pearson\": out[\"pearson\"], \"spearmanr\": out[\"spearmanr\"]}\n",
    "        return compute_metrics\n",
    "    else:\n",
    "        acc = evaluate.load(\"accuracy\")\n",
    "        task_metric = evaluate.load(\"glue\", task_name)\n",
    "        def compute_metrics(eval_pred):\n",
    "            preds, labels = eval_pred\n",
    "            preds = preds.argmax(axis=-1)\n",
    "            out_task = task_metric.compute(predictions=preds, references=labels)\n",
    "            out_acc  = acc.compute(predictions=preds, references=labels)\n",
    "            out_task[\"accuracy\"] = out_acc[\"accuracy\"]\n",
    "            return out_task\n",
    "        return compute_metrics\n",
    "\n",
    "def _to_str(x):\n",
    "    if x is None:\n",
    "        return \"\"\n",
    "    return x if isinstance(x, str) else str(x)\n",
    "\n",
    "def privatize_columns_builder(s1, s2, epsilon):\n",
    "    \"\"\"Rewrite only the text columns, guaranteeing string in/out.\"\"\"\n",
    "    def rewrite_batch(batch):\n",
    "        # Ensure inputs are strings before feeding to DP-MLM\n",
    "        s1_in = [_to_str(x) for x in batch[s1]]\n",
    "        s1_out = []\n",
    "        for x in s1_in:\n",
    "            try:\n",
    "                y = M.dpmlm_rewrite(x, epsilon)\n",
    "            except Exception:\n",
    "                y = x  # fallback to original if rewriter hiccups\n",
    "            # Ensure output is string\n",
    "            s1_out.append(y if isinstance(y, str) else _to_str(y))\n",
    "        priv = {s1: s1_out}\n",
    "\n",
    "        if s2 is not None:\n",
    "            s2_in = [_to_str(x) for x in batch[s2]]\n",
    "            s2_out = []\n",
    "            for x in s2_in:\n",
    "                try:\n",
    "                    y = M.dpmlm_rewrite(x, epsilon)\n",
    "                except Exception:\n",
    "                    y = x\n",
    "                s2_out.append(y if isinstance(y, str) else _to_str(y))\n",
    "            priv[s2] = s2_out\n",
    "\n",
    "        # Keep label untouched\n",
    "        priv[\"label\"] = batch[\"label\"]\n",
    "\n",
    "        return priv\n",
    "    return rewrite_batch\n",
    "\n",
    "def tokenize_builder(tokenizer, s1, s2, is_reg):\n",
    "    \"\"\"Return a batched map fn that tokenizes and sets `labels`.\"\"\"\n",
    "    def tok(batch):\n",
    "        if s2 is None:\n",
    "            enc = tokenizer(batch[s1], truncation=True, max_length=MAX_LENGTH)\n",
    "        else:\n",
    "            enc = tokenizer(batch[s1], batch[s2], truncation=True, max_length=MAX_LENGTH)\n",
    "        enc[\"labels\"] = [float(x) for x in batch[\"label\"]] if is_reg else batch[\"label\"]\n",
    "        return enc\n",
    "    return tok\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 3) Main loop over EPSILON x GLUE (privatize -> fine-tune -> evaluate)\n",
    "# -------------------------------------------------\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
    "data_collator = DataCollatorWithPadding(tokenizer)\n",
    "\n",
    "summary = []  # collect final metrics across epsilons & tasks\n",
    "\n",
    "for epsilon in EPSILONS:\n",
    "    print(f\"\\n================ ε = {epsilon} ================\\n\")\n",
    "\n",
    "    for task, (s1, s2) in TASK_TO_KEYS.items():\n",
    "        print(f\"\\n===== DP-Privatized GLUE Task: {task.upper()} | ε={epsilon} =====\\n\")\n",
    "\n",
    "        # Load original GLUE\n",
    "        ds = load_dataset(\"glue\", task)\n",
    "\n",
    "        # (A) PRIVATIZE with given epsilon\n",
    "        privatize_fn = privatize_columns_builder(s1, s2, epsilon)\n",
    "        ds_priv = {split: data.map(privatize_fn, batched=True) for split, data in ds.items()}\n",
    "\n",
    "        # (B) Setup model/labels\n",
    "        is_reg = (task == \"stsb\")\n",
    "        if is_reg:\n",
    "            num_labels = 1\n",
    "        else:\n",
    "            num_labels = len(ds[\"train\"].features[\"label\"].names)\n",
    "\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            MODEL_NAME, num_labels=num_labels\n",
    "        )\n",
    "\n",
    "        # (C) TOKENIZE privatized text and attach labels\n",
    "        tok_fn = tokenize_builder(tokenizer, s1, s2, is_reg)\n",
    "        remove_cols = ds[\"train\"].column_names  # raw columns to drop post-tokenization\n",
    "        encoded = {k: v.map(tok_fn, batched=True, remove_columns=remove_cols) for k, v in ds_priv.items()}\n",
    "\n",
    "        # (D) Pick eval split(s)\n",
    "        if task == \"mnli\":\n",
    "            eval_main = encoded[\"validation_matched\"]\n",
    "            extra = [(\"validation_mismatched\", encoded[\"validation_mismatched\"])]\n",
    "        else:\n",
    "            eval_main = encoded[\"validation\"]\n",
    "            extra = []\n",
    "\n",
    "        # (E) Train\n",
    "        args = make_training_args(\n",
    "            output_dir=f\"{OUTPUT_BASE}-eps{epsilon}-{task}\",\n",
    "            save_strategy=\"epoch\",\n",
    "            learning_rate=LR,\n",
    "            per_device_train_batch_size=BATCH_SIZE,\n",
    "            per_device_eval_batch_size=BATCH_SIZE,\n",
    "            num_train_epochs=EPOCHS,\n",
    "            weight_decay=WEIGHT_DECAY,\n",
    "            logging_dir=f\"{OUTPUT_BASE}-eps{epsilon}-{task}/logs\",\n",
    "            report_to=\"none\",\n",
    "            load_best_model_at_end=False,\n",
    "            logging_steps=50,\n",
    "            # fp16=True,  # uncomment if your GPU supports it\n",
    "        )\n",
    "\n",
    "        trainer = Trainer(\n",
    "            model=model,\n",
    "            args=args,\n",
    "            #train_dataset=encoded[\"train\"],\n",
    "            #eval_dataset=eval_main,\n",
    "            train_dataset=encoded[\"train\"].shuffle(seed=42).select(range(2000)),\n",
    "            eval_dataset=eval_main.select(range(500)),\n",
    "            tokenizer=tokenizer,\n",
    "            data_collator=data_collator,\n",
    "            compute_metrics=build_compute_metrics(task),\n",
    "        )\n",
    "\n",
    "        trainer.train()\n",
    "\n",
    "        # (F) Evaluate\n",
    "        res_main = trainer.evaluate(eval_dataset=eval_main)\n",
    "        print(f\"\\n--- ε={epsilon} | {task.upper()} | validation ---\")\n",
    "        print(res_main)\n",
    "        summary.append({\"epsilon\": epsilon, \"task\": task, \"split\": \"validation\", **res_main})\n",
    "\n",
    "        for name, split in extra:\n",
    "            res_extra = trainer.evaluate(eval_dataset=split)\n",
    "            print(f\"\\n--- ε={epsilon} | {task.upper()} | {name} ---\")\n",
    "            print(res_extra)\n",
    "            summary.append({\"epsilon\": epsilon, \"task\": task, \"split\": name, **res_extra})\n",
    "\n",
    "# Pretty print a compact summary at the end\n",
    "print(\"\\n================ SUMMARY (all epsilons & tasks) ================\\n\")\n",
    "print(json.dumps(summary, indent=2))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
